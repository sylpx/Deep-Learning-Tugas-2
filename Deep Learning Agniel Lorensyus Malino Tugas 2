{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM21AS00Q7qR88/bEYTMIGa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sylpx/Deep-Learning-Tugas-2/blob/main/Deep%20Learning%20Agniel%20Lorensyus%20Malino%20Tugas%202\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install dan import library\n",
        "!pip install gdown --quiet\n",
        "!pip install tensorflow --quiet\n",
        "\n",
        "import tensorflow as tf\n",
        "import gdown\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import zipfile\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QYfsjhzCQhQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Upload dataset zip\n",
        "file_id = \"13OMC0kC0ibt-cqm1wfpchuV5LZUFHZAO\"\n",
        "url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "gdown.download(url, \"DATASETGAMBAR.zip\", quiet=False)"
      ],
      "metadata": {
        "id": "yywit9GuQi2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Ekstrak dataset\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"DATASETGAMBAR.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")"
      ],
      "metadata": {
        "id": "lk1aEGasQlLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Path awal dataset setelah diekstrak\n",
        "original_dataset_dir = Path(\"dataset/DATASET GAMBAR (resize)\")\n",
        "\n",
        "# Tujuan folder baru\n",
        "base_dir = Path(\"dataset/split_dataset\")\n",
        "train_dir = base_dir / \"train\"\n",
        "val_dir = base_dir / \"val\"\n",
        "test_dir = base_dir / \"test\"\n",
        "\n",
        "# Buat folder train, val, dan test\n",
        "for folder in [train_dir, val_dir, test_dir]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Rasio split\n",
        "val_split = 0.2  # 20% untuk validasi\n",
        "test_split = 0.1 # 10% untuk testing\n",
        "\n",
        "# Untuk setiap kelas\n",
        "for class_dir in original_dataset_dir.iterdir():\n",
        "    if class_dir.is_dir():\n",
        "        images = list(class_dir.glob(\"*\"))\n",
        "        random.shuffle(images)\n",
        "\n",
        "        class_name = class_dir.name\n",
        "\n",
        "        # Hitung jumlah data\n",
        "        total = len(images)\n",
        "        val_count = int(total * val_split)\n",
        "        test_count = int(total * test_split)\n",
        "        train_count = total - val_count - test_count\n",
        "\n",
        "        # Bagi dataset\n",
        "        train_images = images[:train_count]\n",
        "        val_images = images[train_count:train_count+val_count]\n",
        "        test_images = images[train_count+val_count:]\n",
        "\n",
        "        # Buat subfolder kelas di masing-masing folder tujuan\n",
        "        for subset, subset_images in zip(\n",
        "            [train_dir, val_dir, test_dir],\n",
        "            [train_images, val_images, test_images]\n",
        "        ):\n",
        "            class_subset_dir = subset / class_name\n",
        "            class_subset_dir.mkdir(parents=True, exist_ok=True)\n",
        "            for img in subset_images:\n",
        "                shutil.copy(img, class_subset_dir)\n"
      ],
      "metadata": {
        "id": "sjiSdeQnf_ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Definisikan path dataset\n",
        "data_dir = \"dataset/split_dataset\"\n",
        "img_width, img_height = 224, 224  # Ukuran yang lebih umum untuk CNN\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "WsRlNobsQl3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Preprocessing dan split dataset (train/validation)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8,1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'train'),\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'val'),\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(data_dir, 'test'),\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yxmn4OK6Qn-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Hitung class weights\n",
        "class_weights = compute_class_weight('balanced',\n",
        "                                   classes=np.unique(train_generator.classes),\n",
        "                                   y=train_generator.classes)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "print(\"Class Weights:\", class_weights)"
      ],
      "metadata": {
        "id": "DKYpEHK2vlAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Membangun model CNN\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(img_height, img_width, 3)),\n",
        "\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),  # Layer baru\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Dropout(0.4),  # Dropout lebih besar\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.01)),  # Lebih banyak neuron + regularisasi\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(4, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "Y8LJk7twQqEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Kompilasi model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),  # LR lebih kecil\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "w4lPYvwMQrva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Training model dengan EarlyStopping dan ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint(\"best_model.keras\", monitor='val_accuracy', save_best_only=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "epochs = 30  # Naikkan epoch karena ada early stopping\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights  # Tambahkan ini\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z8p1qe99QtSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "q3o-yZPRypyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Visualisasi akurasi dan loss\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A0jZ0xp3QvLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FgRxPgapNyb"
      },
      "outputs": [],
      "source": [
        "# 10. Simpan model\n",
        "model.save(\"model_klasifikasi_4kelas.h5\")\n",
        "print(\"Model saved!\")\n"
      ]
    }
  ]
}